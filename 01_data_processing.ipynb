{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1241ecfb",
   "metadata": {},
   "source": [
    "# The CamVid Dataset\n",
    "\n",
    "<!--- @wandbcode{sagemaker-studio-lab} -->\n",
    "\n",
    "In this notebooks we will pull the Cambridge-driving Labeled Video Database or `CamVid` to train our model. It contains a collection of videos with object class semantic labels, complete with metadata. The database provides ground truth labels that associate each pixel with one of 32 semantic classes.\n",
    "\n",
    "We will upload the full dataset to Weights and Biases as an `wandb.Artifact` first, and then compute some information of what classes are present on each image, and upload the processed dataset as a `wandb.Table`. Doing so enables the user to use the `wandb` UI to visualize and filter images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c50cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c58cbd",
   "metadata": {},
   "source": [
    "Let's check that Weights and Biases is working and we are properly logged into our account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd0dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a90ba52",
   "metadata": {},
   "source": [
    "## Log the raw dataset\n",
    "We will grab a copy of `CamVid` using `fastai`'s `untar_data` method, afterwards we can use the `Artifact.add_dir()` method, and upload the full folder to our wandb workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.CAMVID)\n",
    "codes = np.loadtxt(path/'codes.txt', dtype=str)\n",
    "fnames = get_image_files(path/\"images\")\n",
    "class_labels = {k: str(v) for k, v in enumerate(codes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fcd5d1",
   "metadata": {},
   "source": [
    "- we create a project under `user/project`\n",
    "- If you are working on a team, you can pass the team name to `Entity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e94ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT=\"sagemaker_camvid_demo\"\n",
    "ENTITY=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(\n",
    "    project=PROJECT,\n",
    "    name=\"upload_camvid\",\n",
    "    entity=ENTITY,\n",
    "    job_type=\"upload\",\n",
    "):\n",
    "    artifact = wandb.Artifact(\n",
    "        'camvid-dataset',\n",
    "        type='dataset',\n",
    "        metadata={\n",
    "            \"url\": URLs.CAMVID,\n",
    "            \"class_labels\": class_labels\n",
    "        },\n",
    "        description=(\"The Cambridge-driving Labeled Video Database (CamVid) is the first collection \" \n",
    "                     \"of videos with object class semantic labels, complete with metadata. \" \n",
    "                     \"The database provides ground truth labels that associate each pixel \"\n",
    "                     \"with one of 32 semantic classes.\")\n",
    "    )\n",
    "    artifact.add_dir(path)\n",
    "    wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5e7c8",
   "metadata": {},
   "source": [
    "## Log a `wandb.Table`\n",
    "Let's log a `wandb.Table` with the frequency distribution of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848dd8f0",
   "metadata": {},
   "source": [
    "![](images/camvid_table.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_image = fnames[0]\n",
    "one_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9066b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56904ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/\"labels\").ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b842e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(fn):\n",
    "    return fn.parent.parent/\"labels\"/f\"{fn.stem}_P{fn.suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_func(one_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098e842",
   "metadata": {},
   "source": [
    "let's check that the mapping is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f264399",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([load_image(one_image), load_image(label_func(one_image))], figsize=(15,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad2138",
   "metadata": {},
   "source": [
    "Let's contruct the frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac43fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = load_image(label_func(one_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f328914",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_np = np.array(mask)\n",
    "mask_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e764c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask_np, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b6b98",
   "metadata": {},
   "source": [
    "let's count how many pixels we have per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_pixel_count(mask_data) -> dict:\n",
    "    \"Count pixels on the image for each class\"\n",
    "    (unique, counts) = map(list, np.unique(mask_data, return_counts=True))\n",
    "    frequency_dict = {d:0 for d in class_labels.values()}\n",
    "    for class_id, count in zip(unique, counts):\n",
    "        frequency_dict[class_labels[class_id]] = count\n",
    "    return frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ff937",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_pixel_count(mask_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_ID = 'capecape/sagemaker_camvid_demo/camvid-dataset:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333971be-4aae-43a6-bfeb-f7d72e039b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -1 to log full dataset\n",
    "N_SAMPLES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dataset():\n",
    "    with wandb.init(\n",
    "        project=PROJECT,\n",
    "        name=\"visualize_camvid\",\n",
    "        entity=ENTITY,\n",
    "        job_type=\"data_viz\"\n",
    "    ):\n",
    "        artifact = wandb.use_artifact(ARTIFACT_ID, type='dataset')\n",
    "        artifact_dir = artifact.download()\n",
    "\n",
    "        labels = list(class_labels.values())\n",
    "        \n",
    "        # create an empty wandb.Table\n",
    "        table = wandb.Table(columns=[\"File_Name\", \"Images\"] + labels)\n",
    "        \n",
    "        # get all files\n",
    "        image_files = get_image_files(Path(artifact_dir)/\"images\")[0:N_SAMPLES]\n",
    "        \n",
    "        print(\"Creating Table...\")\n",
    "        for image_file in progress_bar(image_files):\n",
    "            image = Image.open(image_file)\n",
    "            mask_data = np.array(Image.open(label_func(image_file)))\n",
    "            # count of pixels per class\n",
    "            pixel_count = frequency_pixel_count(mask_data)\n",
    "            table.add_data(\n",
    "                str(image_file.name),\n",
    "                wandb.Image(image, masks={\"predictions\": {\"mask_data\": mask_data,\n",
    "                                                          \"class_labels\": class_labels\n",
    "                                                         }}),\n",
    "                *pixel_count.values()\n",
    "            )\n",
    "        wandb.log({\"CamVid_Dataset\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b409d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f284b7",
   "metadata": {},
   "source": [
    "## View the dataset in Weights and Biases workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afb6d3",
   "metadata": {},
   "source": [
    "We get a nice UI to view our images\n",
    "\n",
    "![](images/camvid_mask.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
